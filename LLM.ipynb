{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers, General Python, and Gradio Exercices"
      ],
      "metadata": {
        "id": "SUn3k9jNkIkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "8-x095vjq_B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First part"
      ],
      "metadata": {
        "id": "XCFK2d2xq94T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Install the library `gradio`, and the library `transformers`."
      ],
      "metadata": {
        "id": "pBzQNSnhkP8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMaI5NTYkH2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Create a `gradio` interface connected to GPT-2, like we did during the course. Use a `max_length` of `100`, `10` `num_beams`, `early_stopping=True`, and `no_repeat_ngram_size=2`."
      ],
      "metadata": {
        "id": "-JDsYKw3kpIY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yiKuPcZniGW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Ask the question \"Who are you?\""
      ],
      "metadata": {
        "id": "yHCcfKbwlSoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Why is the model not answering \"I am GPT-2, a ChatBot designed to answer your questions\". Well because, it is not a ChatBot designed to answer your questions yet, for now it is (roughly speaking) just guessing what is most likely next word based on the previous ones."
      ],
      "metadata": {
        "id": "Xv9KDlUamHjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Modify your `gradio` function to include, for every text inputted by the user, a sentence or two to steer the model into behaving like a ChatBot."
      ],
      "metadata": {
        "id": "g2qteHRjmS_E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIvuc1c9lSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Now, you would probably want to hide the sentence(s) you have added to steer the model to behave like you wanted to. Modify `generated_text` so that it only displays the conversation and not the 'wrapper' indicating to the model how to behave."
      ],
      "metadata": {
        "id": "mw_bwLvrndhK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0XRG2rRkZOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "VFjBwurkoigu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Part"
      ],
      "metadata": {
        "id": "Zbh8shXsq7it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Let's now try to use a more recent model: `Llama`. Run the following code:"
      ],
      "metadata": {
        "id": "BmaZtpxCojJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git@refs/pull/25740/head accelerate"
      ],
      "metadata": {
        "id": "z5IVyx2TozxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`git+https://github.com/huggingface/transformers.git@refs/pull/25740/head`: This is a direct link to a specific pull request (`PR #25740`) from the `transformers` repository on GitHub. This means you're not installing the main version of the `transformers` library that's available on PyPI (Python Package Index) but a specific version from this PR.\n",
        "\n",
        "Using `git+` followed by a GitHub URL is a way to tell `pip` to install a Python package directly from a Git repository.\n",
        "\n",
        "This is not too important if you don't fully understand it."
      ],
      "metadata": {
        "id": "J7XJuiCopJFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may need to restart your runtime since we had already installed another version of the `Transformers` library. Do not execute the previous cells otherwise it will ask you to re-restart your runtime. Only execute the cells from this second part of the exercise. Start a GPU Runtime otherwise it will not work. To change it, click on Change runtime type and you will arrive here: https://i.imgur.com/8QjVO0P.png. Select T4 GPU. Why is this useful? This is because on colab, the GPU has more RAM, and you need *a lot* of RAM to load big models.\n",
        "\n",
        "Executing the following cell might take some time. Try to understand every line there is in the meantime."
      ],
      "metadata": {
        "id": "Uh_SBRkMpXiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"codellama/CodeLlama-7b-Instruct-hf\" # \"codellama/CodeLlama-7b-hf\"\n",
        "# Code Llama is a collection of pretrained and fine-tuned generative text models\n",
        "# ranging in scale from 7 billion to 34 billion parameters.\n",
        "# We are here using the 7 billion parameters version.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "prompt = \"Who are you?\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=100,\n",
        "    no_repeat_ngram_size=2,  # avoids repeating the same ngrams\n",
        "    add_special_tokens=False\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "lL0MXCg9pI5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Why are we using `torch.float16`?"
      ],
      "metadata": {
        "id": "-Hcydzvzud8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using float16 (also known as \"half precision\") can lead to faster processing times but might compromise a bit on precision."
      ],
      "metadata": {
        "id": "jz1nISnsuiPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Why is there a for loop:\n",
        "`for seq in sequences:`?\n",
        "\n",
        "What can you deduce about the `sequences` object returned? Prove it using the `type` built-in Python function. **Careful, when you want to re-use Llama, do not re-load the whole pipeline! You don't need to load the pipeline everytime, once it's loaded, you just need to call it.**"
      ],
      "metadata": {
        "id": "fTOaLrbKvSjz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hGYykSnjvg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want the `list` sequences to have more than one element? Print the `k` sequences you asked the model to generate. Comment. If you do not specify `num_beams` > 1, it will not work (for obvious reasons). The diference might be subtle (only one or a few words)."
      ],
      "metadata": {
        "id": "Q9VzY8ICz5v_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8RBoYKTez8-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the same problem as we had with GPT-2. We did to encapsulate the prompt with some \"instructions\"."
      ],
      "metadata": {
        "id": "yBB-gWIQCpqq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyjCWh73Cwmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a lot better!"
      ],
      "metadata": {
        "id": "kS403y4c0FyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's incorporate Llama in `gradio`, and try to benchmark `GPT-2` and `Llama-7b` by creating 10 basic questions on a subject you are good at (it could be translating a sentence from Spanish to English, or a question about physics, ...) and for each question, ask both ChatBots, and say if they were good or not in a dataframe with four columns: Question, Answer, Correct GPT-2 (boolean), Correct Llama.\n",
        "\n",
        "Then save your dataframe as a `.csv` and concatenate all the `.csv` with your teammates. And determine a %accuracy on your benchmark for both models."
      ],
      "metadata": {
        "id": "rNRzL6LMFS1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously the quality of the responses might depend on other factors like `num_beams`, etc ... you can test different `num_beams` as well. But for a unified benchmark across the class, it's better to agree on a set number for each ChatBot."
      ],
      "metadata": {
        "id": "IFBRspdKb8aI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Hp440Y8b6Ex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}