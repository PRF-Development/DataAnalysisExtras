{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Differentiation (SD)\n",
    "### v.s.\n",
    "# Automatic Differentiation (AD)\n",
    "### v.s.\n",
    "# Finite Differences (FD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentiation, Symbolic Differentiation, and Finite Differences are three different approaches used in the field of computational mathematics and computer science for calculating derivatives of mathematical functions.\n",
    "Each approach has its own advantages and disadvantages, and they are used in different contexts depending on the specific requirements of a problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Automatic Differentiation (AutoDiff)**\n",
    "\n",
    "Automatic Differentiation, also known as autodiff or AD, is a computational technique for efficiently and accurately computing derivatives of functions.\n",
    "It is particularly useful for functions that are defined by computer programs or algorithms.\n",
    "AutoDiff is based on the chain rule from calculus.\n",
    "\n",
    "There are two main modes of AutoDiff:\n",
    "\n",
    "- *Forward-mode AutoDiff:* This method computes the derivative of a function with respect to one input variable at a time. It is efficient when the number of input variables is relatively small compared to the number of functions to be evaluated.\n",
    "- *Reverse-mode AutoDiff (Backpropagation):* This method computes the derivatives of all output variables with respect to a single input variable simultaneously. It is particularly efficient when the number of output variables is much larger than the number of input variables, which is common in machine learning and neural network training.\n",
    "\n",
    "AutoDiff is widely used in optimization problems and machine learning, where gradients (derivatives) are essential for training algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
