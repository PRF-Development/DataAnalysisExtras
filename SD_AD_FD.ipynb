{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Differentiation (SD)\n",
    "### v.s.\n",
    "# Automatic Differentiation (AD)\n",
    "### v.s.\n",
    "# Finite Differences (FD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentiation, Symbolic Differentiation, and Finite Differences are three different approaches used in the field of computational mathematics and computer science for calculating derivatives of mathematical functions.\n",
    "Each approach has its own advantages and disadvantages, and they are used in different contexts depending on the specific requirements of a problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Symbolic Differentiation**\n",
    "\n",
    "Symbolic Differentiation involves manipulating mathematical expressions symbolically to obtain their derivatives. Instead of numerical values, it works with algebraic expressions. Symbolic differentiation provides exact derivatives as symbolic expressions.\n",
    "\n",
    "For example, if you have an algebraic expression like $f(x) = x^2 + 3x + 5$, symbolic differentiation will yield $f'(x) = 2x + 3$.\n",
    "\n",
    "Advantages of symbolic differentiation include precision and exactness, which is crucial in certain mathematical and scientific computations. However, it can be computationally expensive and may lead to complex expressions, especially for functions with many variables or intricate forms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Automatic Differentiation (AutoDiff)**\n",
    "\n",
    "Automatic Differentiation, also known as autodiff or AD, is a computational technique for efficiently and accurately computing derivatives of functions.\n",
    "It is particularly useful for functions that are defined by computer programs or algorithms.\n",
    "AutoDiff is based on the chain rule from calculus.\n",
    "\n",
    "There are two main modes of AutoDiff:\n",
    "\n",
    "- *Forward-mode AutoDiff:* This method computes the derivative of a function with respect to one input variable at a time. It is efficient when the number of input variables is relatively small compared to the number of functions to be evaluated.\n",
    "- *Reverse-mode AutoDiff (Backpropagation):* This method computes the derivatives of all output variables with respect to a single input variable simultaneously. It is particularly efficient when the number of output variables is much larger than the number of input variables, which is common in machine learning and neural network training.\n",
    "\n",
    "AutoDiff is widely used in optimization problems and machine learning, where gradients (derivatives) are essential for training algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
