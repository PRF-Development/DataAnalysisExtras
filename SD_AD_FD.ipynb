{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Differentiation (SD)\n",
    "### v.s.\n",
    "# Automatic Differentiation (AD)\n",
    "### v.s.\n",
    "# Finite Differences (FD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentiation, Symbolic Differentiation, and Finite Differences are three different approaches used in the field of computational mathematics and computer science for calculating derivatives of mathematical functions.\n",
    "Each approach has its own advantages and disadvantages, and they are used in different contexts depending on the specific requirements of a problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Symbolic Differentiation**\n",
    "\n",
    "Symbolic Differentiation involves manipulating mathematical expressions symbolically to obtain their derivatives.\n",
    "Instead of numerical values, it works with algebraic expressions. Symbolic differentiation provides exact derivatives as symbolic expressions.\n",
    "\n",
    "For example, if you have an algebraic expression like $f(x) = x^2 + 3x + 5$, symbolic differentiation will yield $f'(x) = 2x + 3$.\n",
    "\n",
    "Advantages of symbolic differentiation include precision and exactness, which is crucial in certain mathematical and scientific computations.\n",
    "However, it can be computationally expensive and may lead to complex expressions, especially for functions with many variables or intricate forms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Automatic Differentiation (AutoDiff)**\n",
    "\n",
    "Automatic Differentiation, also known as autodiff or AD, is a computational technique for efficiently and accurately computing derivatives of functions.\n",
    "It is particularly useful for functions that are defined by computer programs or algorithms.\n",
    "AutoDiff is based on the chain rule from calculus.\n",
    "\n",
    "There are two main modes of AutoDiff:\n",
    "\n",
    "- *Forward-mode AutoDiff:* This method computes the derivative of a function with respect to one input variable at a time. It is efficient when the number of input variables is relatively small compared to the number of functions to be evaluated.\n",
    "- *Reverse-mode AutoDiff (Backpropagation):* This method computes the derivatives of all output variables with respect to a single input variable simultaneously. It is particularly efficient when the number of output variables is much larger than the number of input variables, which is common in machine learning and neural network training.\n",
    "\n",
    "AutoDiff is widely used in optimization problems and machine learning, where gradients (derivatives) are essential for training algorithms like gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Finite Differences**\n",
    "\n",
    "Finite Differences is a numerical method for approximating derivatives by using the values of a function at discrete points.\n",
    "It is a simple and intuitive approach but is generally less accurate than AutoDiff or Symbolic Differentiation.\n",
    "\n",
    "The basic idea is to calculate the derivative by dividing the change in the function's value ($\\delta y$) by the change in the input variable ($\\delta x$).\n",
    "There are several finite difference schemes, including forward differences, backward differences, and central differences, depending on how the neighboring points are chosen.\n",
    "\n",
    "For example, the forward difference approximation for the derivative of a function *f(x)* at a point $x$ is given by:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x + \\delta x) - f(x)}{\\delta x}$$\n",
    "\n",
    "Finite Differences are straightforward to implement and can be used when you have access to the function's values but not its analytical expression.\n",
    "They are commonly used in numerical analysis and scientific computing for problems where symbolic differentiation is impractical or too costly.\n",
    "It is also used when the function of interest is a \"black box\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function $f(x) = 3*(2x+1)^2$ as an example for each technique in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For symbolic differentiation, you can use a symbolic math library like `SymPy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative using Symbolic Differentiation: 24*x + 12\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the symbolic variable and function\n",
    "x = sp.symbols('x')\n",
    "f = 3 * (2 * x + 1)**2\n",
    "\n",
    "# Compute the derivative symbolically\n",
    "derivative_symbolic = sp.diff(f, x)\n",
    "\n",
    "print(\"Derivative using Symbolic Differentiation:\", derivative_symbolic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For AutoDiff in Python, you can use libraries like `TensorFlow` or `PyTorch`.\n",
    "Here's an example using `PyTorch` (we will use `PyTorch` in this course):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative using AutoDiff: 60.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input variable and function\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "f = 3 * (2 * x + 1)**2\n",
    "\n",
    "# Compute the derivative using AutoDiff\n",
    "f.backward()\n",
    "\n",
    "# The derivative is stored in x.grad\n",
    "derivative_auto_diff = x.grad.item()\n",
    "\n",
    "print(\"Derivative using AutoDiff:\", derivative_auto_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For finite differences, you can approximate the derivative numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative using Finite Differences: 60.01199999998619\n"
     ]
    }
   ],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    return 3 * (2 * x + 1)**2\n",
    "\n",
    "# Choose a small delta x\n",
    "delta_x = 0.001\n",
    "\n",
    "# Calculate the derivative using finite differences (forward differences)\n",
    "x_value = 2.0\n",
    "derivative_finite_diff = (f(x_value + delta_x) - f(x_value)) / delta_x\n",
    "\n",
    "print(\"Derivative using Finite Differences:\", derivative_finite_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is very different ofr the 3 techniques:\n",
    "- SD gives us an expression\n",
    "- AD gives us the exact value, for a given input\n",
    "- FD gives us an approximative value, for a given input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the computation times of the three techniques for the same example function ($f(x) = 3*(2x+1)^2$) using the `time` module of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative using Symbolic Differentiation: 24*x + 12\n",
      "Derivative using Automatic Differentiation: 60.0\n",
      "Derivative using Finite Differences: 60.01199999998619\n",
      "Computation time for Symbolic Differentiation: 0.001008749008178711\n",
      "Computation time for Automatic Differentiation: 0.0009920597076416016\n",
      "Computation time for Finite Differences: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy as sp\n",
    "import time\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return 3 * (2 * x + 1)**2\n",
    "\n",
    "# Method 1: Symbolic Differentiation\n",
    "x_symbolic = sp.symbols('x')\n",
    "f_symbolic = f(x_symbolic)\n",
    "start_time = time.time()\n",
    "derivative_symbolic = sp.diff(f_symbolic, x_symbolic)\n",
    "symbolic_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Automatic Differentiation\n",
    "x_auto_diff = torch.tensor([2.0], requires_grad=True)\n",
    "start_time = time.time()\n",
    "f_auto_diff = f(x_auto_diff)\n",
    "f_auto_diff.backward()\n",
    "derivative_auto_diff = x_auto_diff.grad.item()\n",
    "auto_diff_time = time.time() - start_time\n",
    "\n",
    "# Method 3: Finite Differences\n",
    "x_value = 2.0\n",
    "delta_x = 0.001\n",
    "start_time = time.time()\n",
    "derivative_finite_diff = (f(x_value + delta_x) - f(x_value)) / delta_x\n",
    "finite_diff_time = time.time() - start_time\n",
    "\n",
    "print(\"Derivative using Symbolic Differentiation:\", derivative_symbolic)\n",
    "print(\"Derivative using Automatic Differentiation:\", derivative_auto_diff)\n",
    "print(\"Derivative using Finite Differences:\", derivative_finite_diff)\n",
    "\n",
    "print(\"Computation time for Symbolic Differentiation:\", symbolic_time)\n",
    "print(\"Computation time for Automatic Differentiation:\", auto_diff_time)\n",
    "print(\"Computation time for Finite Differences:\", finite_diff_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that SD is the slowest, followed by AD, and FD is the fastest.\n",
    "- For more complex functions, SD would much slower than AD (for simple example like this one, they're about the same).\n",
    "- FD is always very fast, but it gives an approximation, and may not be stable on more complex examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one multi-variables example: \n",
    "$$f(x, y) = x^2y^3 + 3(x-1)^2y + (y+1)^2/x$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
