{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size, Optimizers, Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study the behaviour of batch size, optimizers, and loss on a linear regression, as it makes the parameters easy to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake data & basic linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some random points roughly aligned, so we can perform a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "XS = np.random.rand(100, 1)\n",
    "YS = 1 + 2 * XS + .1 * np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(XS, YS)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression is a simple model, where $y_{pred} = a*x+b$.\n",
    "The loss (function we try to minimize) is usually the mean squared error $\\mathcal{L}(a,b) = \\sum_{i=1}^N ({y_{pred}}_i-y_i)^2 = \\sum_{i=1}^N ((ax_i+b)-y_i)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that calculates the loss for $a \\in \\left[0,2\\right]$ and $b \\in \\left[-1,3\\right]$ (with a grid of $100 \\times 100$ points), and plot the loss on that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_map(xs, ys):\n",
    "    aa = np.linspace(0,4, 100)\n",
    "    bb = np.linspace(-1,3, 100)\n",
    "    A, B = np.meshgrid(aa, bb)\n",
    "    L = np.zeros((100,100))\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            a = A[i,j]\n",
    "            b = B[i,j]\n",
    "            yhat = a*xs + b\n",
    "            L[i,j] = np.log(np.mean((ys-yhat)**2))\n",
    "    return L\n",
    "\n",
    "L = loss_map(XS, YS)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a tensor dataset from our points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS_T = torch.from_numpy(XS).float()\n",
    "YS_T = torch.from_numpy(YS).float()\n",
    "dataset = TensorDataset(XS_T, YS_T)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `nn.Module` (a model) that is a linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.a * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of training for our linear model.\n",
    "On top of training, this cell also stores the evolution of values of $a$ and $b$ over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=100)\n",
    "torch.manual_seed(42)\n",
    "model = LinearModel().to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "a_s = [model.a.item()]\n",
    "b_s = [model.b.item()]\n",
    "t_s = [0]\n",
    "t = time.time()\n",
    "for epoch in range(1000):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_batch_hat = model(x_batch)\n",
    "        loss = loss_fn(y_batch, y_batch_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        a_s.append(model.a.item())\n",
    "        b_s.append(model.b.item())\n",
    "        t_s.append(time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a code to plot the evolution of $a$ and $b$ over time, and in the loss space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(t_s, a_s, '+', label='a')\n",
    "plt.plot(t_s, b_s, '.', label='b')\n",
    "plt.legend()\n",
    "plt.ylim(min(0, min(a_s),min(b_s)), max(2.5, max(a_s),max(b_s)))\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.plot(a_s, b_s, \"k.\")\n",
    "plt.plot(a_s, b_s, 'r', lw=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(-1, 3)\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two cells above, create a function `plot_training` that create the two plots above.\n",
    "The function should have arguments `n_epochs` (the number of epochs), `lr` (the learning rate), and `batch_size` (the size of the batches).\n",
    "This function should also return the value of the parameters $a$ and $b$ at the end of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(n_epochs, lr, batch_size):\n",
    "    pass\n",
    "\n",
    "plot_training(n_epochs=1000, lr=0.05, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try `n_epochs=10`, `lr=0.1`, `batch_size=100` this will be our basis, to be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of epoch untill convegrence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `lr=0.5`. How many epoch do you now need to reach convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we use even less epochs by increasing the learning rate even more?\n",
    "\n",
    "Let `lr=1`. Can you reach convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `lr=0.5` again; \n",
    "Let's modify the batch size by putting `batch_size=5`.\n",
    "\n",
    "How many epochs are now needed to reach convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution appears to be very unstable, let's lower the learning rate in order to have more stability.\n",
    "\n",
    "Put `lr=0.05`.\n",
    "How many epochs are now needed to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we lower even more the learning rate?\n",
    "\n",
    "Put `lr=0.005`.\n",
    "How many epochs are now needed to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that:\n",
    "- the larger the learning rate, the faster we converge\n",
    "- a learning rate too large causes divergence\n",
    "- a learning rate too low will converge, but (extremely) slowly;\n",
    "- the bigger the batch size, the more stable our solution is\n",
    "- a batch size too large may not fit in our memory (here, the full dataset is just two lists of 100 points, but if the dataset was composed of 10000 or 100000 images, then we would be forced to use a smaller batch size)\n",
    "- a batch size too low makes the gradient very unstable, and therefore convergence is harder to reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of thumb are:\n",
    "* When you lower the batch size, lower the learning rate aw well\n",
    "* When you increase the batch size, you can afford to increase the learning rate;\n",
    "* Batch size as large as possible, given you memory (so that we increase stability of the gradient descend)\n",
    "* Try various learning rates by hand, pick the largest one that does not diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add momentum.\n",
    "\n",
    "Modify your `plot_training` function to incorporate a new parameter: `momentum`, that you give to SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(n_epochs, lr, batch_size, momentum=0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `n_epochs=150`, `lr=0.2`, `batch_size=100`.\n",
    "\n",
    "Test how many epochs are needed to converge for the following momentum values:\n",
    "- `momentum=0`\n",
    "- `momentum=0.3`\n",
    "- `momentum=0.7`\n",
    "- `momentum=0.9`\n",
    "- `momentum=1`\n",
    "- `momentum=1.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule of thumb: **Typically, we set momentum in $\\left[ 0.7, 0.9 \\right]$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is an optimizer that adapts the learning rate ot each parameter. It also has a momentum included by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change your `plot_hyper` function to include a parameter `adam` that may be set to true/false if we want to use Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(n_epochs, lr, batch_size, momentum=0, adam=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an optimization with adam optimizer\n",
    "```\n",
    "plot_training(n_epochs=100, lr=0.2, batch_size=100, adam=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam has two parameters for the momentum (the \"betas\").\n",
    "You can try to play with it, but keep in mind that 99.9% of the time, we use the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule of thumb: **Most people are to lazy to find the perfect momentum; so most of the time, we just use Adam.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function does not change the speed of convergence, but the point to which we converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an `error_power` parameter to our `calc_loss` function.\n",
    "By default, we take the mean square error, but settin this parameter to 1 should take the mean absolute error, and setting it to 3 should take the mean absolute cubic error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_map(xs, ys, pow_err=2):\n",
    "    aa = np.linspace(0,4, 100)\n",
    "    bb = np.linspace(-1,3, 100)\n",
    "    A, B = np.meshgrid(aa, bb)\n",
    "    L = np.zeros((100,100))\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            a = A[i,j]\n",
    "            b = B[i,j]\n",
    "            yhat = a*xs + b\n",
    "            L[i,j] = np.log(np.mean(np.abs(ys-yhat)**pow_err))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to plot MSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = loss_map(XS, YS, 1)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.title('MAE')\n",
    "plt.show()\n",
    "\n",
    "L = loss_map(XS, YS, 2)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.title('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change a bit the data, so that we can see better what changes.\n",
    "Rerun using the following $XS$ nad $YS$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "XS = np.random.rand(100, 1)\n",
    "XS[:10] += 0.8\n",
    "YS = 1 + 2 * XS + .1 * np.random.randn(100, 1)\n",
    "YS[:10] -= 1.5\n",
    "XS_T = torch.from_numpy(XS).float()\n",
    "YS_T = torch.from_numpy(YS).float()\n",
    "dataset = TensorDataset(XS_T, YS_T)\n",
    "\n",
    "plt.scatter(XS, YS)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = loss_map(XS, YS, 1)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.title('MAE')\n",
    "plt.show()\n",
    "\n",
    "L = loss_map(XS, YS, 2)\n",
    "plt.imshow(L, extent=[0, 4, -1, 3])\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.title('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiy the `plot_training` function, to include the parameter `error_power`. Also, rename the function `training` and stop plotting, as we now just want to compare the values of $a$ and $b$ (after the model has converged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(n_epochs, lr, batch_size, pow_err=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute optimized parameters $a$ and $b$ for `pow_err` of $1$, $2$, and $5$.\n",
    "Plot the three linear models together with a scatter of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater the power, the more the model is affected by extreme values.\n",
    "Depending on the application, this may be more or less desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule of thumbs: **Use MSE** (i.e. `pow_err=2`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
